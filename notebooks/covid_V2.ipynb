{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b25fa5-17f5-4145-a1fb-e68ab083ff58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create initial vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f944d5-f884-4259-8edf-d520fc2dbbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "Total number of documents: 2019\n",
      "Creating passages\n",
      "  0%|                                                  | 0/2019 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 2019/2019 [02:56<00:00, 11.45it/s]\n",
      "Total number of passages: 138891\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 8397\n",
      "Creating vector store\n",
      "Load model embedding : BAAI/bge-small-en-v1.5\n",
      "Using device: cuda\n",
      "Generando embeddings: 100%|███████████████████████| 5/5 [00:07<00:00,  1.47s/it]\n",
      "✅ Índice FAISS creado exitosamente.\n",
      "💾 Vector store saved in ../vector_stores/covid/base_vs_covid_150_20\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"covid\" \\\n",
    "  --emb_model \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 2048 \\\n",
    "  --output_dir \"../vector_stores/covid/base_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209bb917-fc14-48e2-8194-7ad216ca072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : BAAI/bge-small-en-v1.5\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/covid/base_vs_covid_150_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|███████████████████████████████| 1/1 [00:00<00:00, 107.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['We carried out an association study of DC-SIGNR polymorphism in 197 infants born to untreated HIV-1-infected mothers recruited in Harare, Zimbabwe. Among them, 97 infants were HIV-1-infected and 100 infants remained uninfected. Of the 97 HIV-1-infected infants, 57 were infected IU, 11 were infected IP, and 17 were infected PP. Timing of infection was not determined for 12 HIV-1-infected infants. Baseline characteristics of mothers and infants are presented in Table 1 . Maternal age and CD4 cell count, child sex, mode of delivery, duration of membrane rupture and gestational age were similar among all groups. However, maternal viral load',\n",
       "   'The single strongest risk factor for pneumonia is HIV infection, which is especially prevalent in children in sub-Saharan Africa. HIV-infected children have 6 times increased odds of developing severe pneumonia or of death compared to HIV-uninfected children [52] . Since the effective prevention of mother-to-child transmission of HIV, there is a growing population of HIV-exposed children who are uninfected; their excess risk of pneumonia, compared to HIV unexposed children, has been described as 1.3-to 3.4-fold higher [53] [54] [55] [56] [57] .'],\n",
       "  array([0.7539171 , 0.74739456], dtype=float32))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/covid/base_vs_covid_150_20\")\n",
    "results = vector_store.buscar_por_batches(['What is the main cause of HIV-1 infection in children?'],2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea2d78bb-8ccb-4b07-a2da-41dada541232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en-v1.5'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.embedding_model_name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af04f5-8a4a-428f-8966-531ce073b575",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc54f7b-3dab-43da-a564-e0d3f5304c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset splits for covid\n",
      "Train: 1292\n",
      "Val: 323\n",
      "Test: 404\n",
      "Datasets loaded and prepared.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0332f16afc30441bac77bdb2784a1350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1292 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ce691033a7438ba7f4218b1f5741a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9969e129f2c44d8298cfc6a01d7aa494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded and prepared.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_id': 1891,\n",
       " 'question': 'As of 26 January 2020, what countries had sporadic cases?',\n",
       " 'relevant_docs': '0. As of 26 January 2020, the still ongoing outbreak had resulted in 2066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (WHO) news release via https://www.who.int/csr/don/en/ from 14 to 21 Jan'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.data_for_train_emb import load_and_prepare_datasets\n",
    "train_dataset, val_dataset, test_dataset = load_and_prepare_datasets('covid')\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c6e0fd-4085-46f6-b592-acbe3a2f9f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Starting main process...\n",
      "Loading dataset splits for covid\n",
      "Train: 1292\n",
      "Val: 323\n",
      "Test: 404\n",
      "Datasets loaded and prepared.\n",
      "Datasets loaded and prepared.\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/solving_problems/test_sbbd/notebooks/wandb/run-20250430_171033-8x7k36ta\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbge-small-covid_10e_128bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/8x7k36ta\u001b[0m\n",
      "Loading model: BAAI/bge-small-en-v1.5\n",
      "Model and loss function initialized.\n",
      "Configuring training arguments...\n",
      "Training arguments configured.\n",
      "Starting training process...\n",
      "{'loss': 1.6314, 'grad_norm': 3.8557729721069336, 'learning_rate': 4.988679806432712e-05, 'epoch': 1.36}\n",
      " 14%|█████▋                                    | 15/110 [00:03<00:18,  5.07it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7266051769256592, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.739938080495356, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.8730650154798761, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.9164086687306502, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9535603715170279, 'eval_telecom-ir-eval_cosine_precision@1': 0.739938080495356, 'eval_telecom-ir-eval_cosine_recall@1': 0.739938080495356, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8511960723698525, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8018575851393189, 'eval_telecom-ir-eval_cosine_mrr@5': 0.8123839009287929, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8178387144331416, 'eval_telecom-ir-eval_cosine_map@100': 0.8190042723287869, 'eval_runtime': 0.5981, 'eval_samples_per_second': 540.057, 'eval_steps_per_second': 5.016, 'epoch': 1.36}\n",
      " 14%|█████▋                                    | 15/110 [00:03<00:18,  5.07it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.35it/s]\u001b[A\n",
      "{'loss': 0.6731, 'grad_norm': 3.0493714809417725, 'learning_rate': 4.6031338320779534e-05, 'epoch': 2.73}\n",
      " 27%|███████████▍                              | 30/110 [00:07<00:15,  5.19it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5168565511703491, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.7925696594427245, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.9071207430340558, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.934984520123839, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9659442724458205, 'eval_telecom-ir-eval_cosine_precision@1': 0.7925696594427245, 'eval_telecom-ir-eval_cosine_recall@1': 0.7925696594427245, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8837240966203733, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8462332301341591, 'eval_telecom-ir-eval_cosine_mrr@5': 0.8530443756449951, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8570150867364492, 'eval_telecom-ir-eval_cosine_map@100': 0.8584156288934561, 'eval_runtime': 0.5859, 'eval_samples_per_second': 551.251, 'eval_steps_per_second': 5.12, 'epoch': 2.73}\n",
      " 27%|███████████▍                              | 30/110 [00:07<00:15,  5.19it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.48it/s]\u001b[A\n",
      "{'loss': 0.3454, 'grad_norm': 2.652418851852417, 'learning_rate': 3.7500000000000003e-05, 'epoch': 4.09}\n",
      " 41%|█████████████████▏                        | 45/110 [00:10<00:10,  6.10it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.4398502707481384, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.7987616099071208, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.9164086687306502, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.9473684210526315, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9721362229102167, 'eval_telecom-ir-eval_cosine_precision@1': 0.7987616099071208, 'eval_telecom-ir-eval_cosine_recall@1': 0.7987616099071208, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8898084936590193, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8529411764705884, 'eval_telecom-ir-eval_cosine_mrr@5': 0.8599071207430341, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8630202958376334, 'eval_telecom-ir-eval_cosine_map@100': 0.8642669033662879, 'eval_runtime': 0.5945, 'eval_samples_per_second': 543.332, 'eval_steps_per_second': 5.046, 'epoch': 4.09}\n",
      " 41%|█████████████████▏                        | 45/110 [00:11<00:10,  6.10it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.40it/s]\u001b[A\n",
      "{'loss': 0.2325, 'grad_norm': 1.933361530303955, 'learning_rate': 2.6189547895593562e-05, 'epoch': 5.45}\n",
      " 55%|██████████████████████▉                   | 60/110 [00:14<00:09,  5.19it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.4409410357475281, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.7987616099071208, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.9164086687306502, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.9442724458204335, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9659442724458205, 'eval_telecom-ir-eval_cosine_precision@1': 0.7987616099071208, 'eval_telecom-ir-eval_cosine_recall@1': 0.7987616099071208, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8892766430867954, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8539731682146544, 'eval_telecom-ir-eval_cosine_mrr@5': 0.8606295149638803, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8639097744360902, 'eval_telecom-ir-eval_cosine_map@100': 0.865580485950781, 'eval_runtime': 0.5879, 'eval_samples_per_second': 549.4, 'eval_steps_per_second': 5.103, 'epoch': 5.45}\n",
      " 55%|██████████████████████▉                   | 60/110 [00:15<00:09,  5.19it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.62it/s]\u001b[A\n",
      "{'loss': 0.1762, 'grad_norm': 1.8206455707550049, 'learning_rate': 1.4614624674952842e-05, 'epoch': 6.82}\n",
      " 68%|████████████████████████████▋             | 75/110 [00:18<00:06,  5.26it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.4345478117465973, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.7894736842105263, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.9164086687306502, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.9473684210526315, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9690402476780186, 'eval_telecom-ir-eval_cosine_precision@1': 0.7894736842105263, 'eval_telecom-ir-eval_cosine_recall@1': 0.7894736842105263, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8868124216576491, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8488132094943242, 'eval_telecom-ir-eval_cosine_mrr@5': 0.8563983488132095, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8595803233574133, 'eval_telecom-ir-eval_cosine_map@100': 0.8610131730999645, 'eval_runtime': 0.5912, 'eval_samples_per_second': 546.389, 'eval_steps_per_second': 5.075, 'epoch': 6.82}\n",
      " 68%|████████████████████████████▋             | 75/110 [00:18<00:06,  5.26it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.51it/s]\u001b[A\n",
      "{'loss': 0.142, 'grad_norm': 2.316438674926758, 'learning_rate': 5.348672631430318e-06, 'epoch': 8.18}\n",
      " 82%|██████████████████████████████████▎       | 90/110 [00:22<00:03,  5.90it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.42904362082481384, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.7956656346749226, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.9195046439628483, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.9504643962848297, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9721362229102167, 'eval_telecom-ir-eval_cosine_precision@1': 0.7956656346749226, 'eval_telecom-ir-eval_cosine_recall@1': 0.7956656346749226, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8904933406000962, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8534571723426213, 'eval_telecom-ir-eval_cosine_mrr@5': 0.860732714138287, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8636259767064721, 'eval_telecom-ir-eval_cosine_map@100': 0.8647681885294026, 'eval_runtime': 0.5869, 'eval_samples_per_second': 550.304, 'eval_steps_per_second': 5.111, 'epoch': 8.18}\n",
      " 82%|██████████████████████████████████▎       | 90/110 [00:22<00:03,  5.90it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.49it/s]\u001b[A\n",
      "{'loss': 0.1595, 'grad_norm': 2.1365575790405273, 'learning_rate': 4.517825684323324e-07, 'epoch': 9.55}\n",
      " 95%|███████████████████████████████████████▏ | 105/110 [00:25<00:00,  5.72it/s]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.42965224385261536, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.7956656346749226, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.9164086687306502, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.9504643962848297, 'eval_telecom-ir-eval_cosine_accuracy@10': 0.9690402476780186, 'eval_telecom-ir-eval_cosine_precision@1': 0.7956656346749226, 'eval_telecom-ir-eval_cosine_recall@1': 0.7956656346749226, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.8892818751677287, 'eval_telecom-ir-eval_cosine_mrr@3': 0.8524251805985553, 'eval_telecom-ir-eval_cosine_mrr@5': 0.8603199174406605, 'eval_telecom-ir-eval_cosine_mrr@10': 0.8629428964568285, 'eval_telecom-ir-eval_cosine_map@100': 0.8643417002682758, 'eval_runtime': 0.6018, 'eval_samples_per_second': 536.723, 'eval_steps_per_second': 4.985, 'epoch': 9.55}\n",
      " 95%|███████████████████████████████████████▏ | 105/110 [00:26<00:00,  5.72it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 29.37it/s]\u001b[A\n",
      "{'train_runtime': 28.4588, 'train_samples_per_second': 453.99, 'train_steps_per_second': 3.865, 'train_loss': 0.46439802158962595, 'epoch': 10.0}\n",
      "100%|█████████████████████████████████████████| 110/110 [00:28<00:00,  3.86it/s]\n",
      "Training completed.\n",
      "Saving the trained model...\n",
      "Model saved successfully.\n",
      "Process completed successfully.\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mbge-small-covid_10e_128bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/8x7k36ta\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250430_171033-8x7k36ta/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/train_embedding.py \\\n",
    "  --name_dataset \"covid\" \\\n",
    "  --model_name \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --new_model_name \"bge-small-covid\" \\\n",
    "  --epochs 10 \\\n",
    "  --batch_size 128 \\\n",
    "  --output_dir \"../models/covid/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f932a-3261-49ad-a1e2-e2e80ef9168d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43de0efa-29fd-4d06-9571-b45672a92481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos...\n",
      "Models loaded\n",
      "Loading datasets...\n",
      "Loading dataset splits for covid\n",
      "Train: 1292\n",
      "Val: 323\n",
      "Test: 404\n",
      "Datasets loaded and prepared.\n",
      "Datasets loaded and prepared.\n",
      "Loaded dataset\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "Evaluating models\n",
      "Save results...\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/evaluate_embedding.py \\\n",
    "  --name_dataset \"covid\" \\\n",
    "  --output_dir \"../results/covid/\" \\\n",
    "  --models_dir \"../models/covid/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb0fb4-e469-4c9b-b642-190dd229f0b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## create new vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef8dd0d-ffbc-4cad-bb6b-b581ef0a8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "Total number of documents: 2019\n",
      "Creating passages\n",
      "  0%|                                                  | 0/2019 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████| 2019/2019 [02:56<00:00, 11.44it/s]\n",
      "Total number of passages: 138891\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 8397\n",
      "Creating vector store\n",
      "Load model embedding : ../models/covid/embedding/bge-small-covid_10e_128bs\n",
      "Using device: cuda\n",
      "Generando embeddings: 100%|███████████████████████| 5/5 [00:07<00:00,  1.48s/it]\n",
      "✅ Índice FAISS creado exitosamente.\n",
      "💾 Vector store saved in ../vector_stores/covid/ft_vs_covid_150_20\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"covid\" \\\n",
    "  --emb_model \"../models/covid/embedding/bge-small-covid_10e_128bs\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 2048 \\\n",
    "  --output_dir \"../vector_stores/covid/ft_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fabfde37-0c36-40af-9e5b-5c32e3dc7460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : ../models/covid/embedding/bge-small-covid_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/covid/ft_vs_covid_150_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|███████████████████████████████| 1/1 [00:00<00:00, 111.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['The single strongest risk factor for pneumonia is HIV infection, which is especially prevalent in children in sub-Saharan Africa. HIV-infected children have 6 times increased odds of developing severe pneumonia or of death compared to HIV-uninfected children [52] . Since the effective prevention of mother-to-child transmission of HIV, there is a growing population of HIV-exposed children who are uninfected; their excess risk of pneumonia, compared to HIV unexposed children, has been described as 1.3-to 3.4-fold higher [53] [54] [55] [56] [57] .',\n",
       "   'We carried out an association study of DC-SIGNR polymorphism in 197 infants born to untreated HIV-1-infected mothers recruited in Harare, Zimbabwe. Among them, 97 infants were HIV-1-infected and 100 infants remained uninfected. Of the 97 HIV-1-infected infants, 57 were infected IU, 11 were infected IP, and 17 were infected PP. Timing of infection was not determined for 12 HIV-1-infected infants. Baseline characteristics of mothers and infants are presented in Table 1 . Maternal age and CD4 cell count, child sex, mode of delivery, duration of membrane rupture and gestational age were similar among all groups. However, maternal viral load'],\n",
       "  array([0.5734177, 0.5191275], dtype=float32))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/covid/ft_vs_covid_150_20\")\n",
    "results = vector_store.buscar_por_batches(['What is the main cause of HIV-1 infection in children?'],2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ef6c25-af06-4f09-b066-ee7b5d1e06cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/covid/embedding/bge-small-covid_10e_128bs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.embedding_model_name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf24f54-ff30-4b85-89e2-cd91d2d6e5aa",
   "metadata": {},
   "source": [
    "## Phi fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567630a9-8623-4a9f-a132-3633ac809246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : ../models/covid/embedding/bge-small-covid_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/covid/ft_vs_covid_150_20\n",
      "Creating dataset for covid\n",
      "Loading dataset splits for covid\n",
      "Train: 1292\n",
      "Val: 323\n",
      "Test: 404\n",
      "Datasets loaded and prepared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|████████████████████████████████████████████████████████████████████| 162/162 [00:02<00:00, 80.48it/s]\n",
      "🔍 Buscando: 100%|██████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 94.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.data_for_train_phi import get_dataset_for_train_phi\n",
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/covid/ft_vs_covid_150_20\")\n",
    "train_ds, test_ds = get_dataset_for_train_phi('covid', True, vector_store,4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1746e970-b7c4-4217-b91f-5c036ef911ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct:  Using the information in the context, answer the question as concisely and faithfully as possible.If the context does not provide enough information to answer confidently, answer based on the most likely interpretation from the given text.\n",
      "\n",
      "Context:\n",
      "\n",
      "Document 0:. As of 26 January 2020, the still ongoing outbreak had resulted in 2066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (WHO) news release via https://www.who.int/csr/don/en/ from 14 to 21 January 2020\n",
      "Document 1:The first three cases detected were reported in France on 24 January 2020 and had onset of symptoms on 17, 19 and 23 January respectively [10] . The first death was reported on 15 February in France. As at 21 February, nine countries had reported cases ( Figure) : Belgium (1), Finland (1), France (12), Germany (16), Italy (3), Russia (2), Spain (2), Sweden (1) and the UK (9 -not included further).\n",
      "Document 2:. Nonetheless, the virus spread from Wuhan to other cities in China and outside the country. By February 4, 2020, a total of 23 locations outside mainland China reported cases, 22 of which reported imported cases; Spain reported a case caused by secondary transmission (3).\n",
      "Document 3:066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (W\n",
      "Document 4:As at 09:00 on 21 February, few COVID-19 cases had been detected in Europe compared with Asia. However the situation is rapidly developing, with a large outbreak recently identified in northern Italy, with transmission in several municipalities and at least two deaths [18] . As at 5 March 2020, there are 4,250 cases including 113 deaths reported among 38 countries in the WHO European region [19] .\n",
      "\n",
      "Question:\n",
      "As of 26 January 2020, what countries had sporadic cases?\n",
      "\n",
      "Output:\n",
      "were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71330a5-b3ac-40b5-bfae-5d8222f77b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/solving_problems/test_sbbd/notebooks/wandb/run-20250430_232204-hf76a9qp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi_2_rag_covid_k3_5e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/hf76a9qp\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "Load model embedding : ../models/covid/embedding/bge-small-covid_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/covid/ft_vs_covid_150_20\n",
      "Using k = 3 passages\n",
      "Creating dataset for covid\n",
      "Loading dataset splits for covid\n",
      "Train: 1292\n",
      "Val: 323\n",
      "Test: 404\n",
      "Datasets loaded and prepared.\n",
      "🔍 Buscando: 100%|████████████████████████████| 162/162 [00:01<00:00, 84.63it/s]\n",
      "🔍 Buscando: 100%|██████████████████████████████| 41/41 [00:00<00:00, 96.70it/s]\n",
      "Input Example:\n",
      "Instruct:  Using the information in the context, answer the question as concisely and faithfully as possible.If the context does not provide enough information to answer confidently, answer based on the most likely interpretation from the given text.\n",
      "\n",
      "Context:\n",
      "\n",
      "Document 0:. As of 26 January 2020, the still ongoing outbreak had resulted in 2066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (WHO) news release via https://www.who.int/csr/don/en/ from 14 to 21 January 2020\n",
      "Document 1:0. As of 26 January 2020, the still ongoing outbreak had resulted in 2066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (WHO) news release via https://www.who.int/csr/don/en/ from 14 to 21 Jan\n",
      "Document 2:The first three cases detected were reported in France on 24 January 2020 and had onset of symptoms on 17, 19 and 23 January respectively [10] . The first death was reported on 15 February in France. As at 21 February, nine countries had reported cases ( Figure) : Belgium (1), Finland (1), France (12), Germany (16), Italy (3), Russia (2), Spain (2), Sweden (1) and the UK (9 -not included further).\n",
      "Document 3:. Nonetheless, the virus spread from Wuhan to other cities in China and outside the country. By February 4, 2020, a total of 23 locations outside mainland China reported cases, 22 of which reported imported cases; Spain reported a case caused by secondary transmission (3).\n",
      "\n",
      "Question:\n",
      "As of 26 January 2020, what countries had sporadic cases?\n",
      "\n",
      "Output:\n",
      "were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.69it/s]\n",
      "Training\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "🟢 No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 0.3375, 'grad_norm': 0.32465019822120667, 'learning_rate': 0.00019267614527653488, 'epoch': 0.7692307692307693}\n",
      "{'eval_loss': 0.32941320538520813, 'eval_runtime': 29.2309, 'eval_samples_per_second': 11.05, 'eval_steps_per_second': 1.129, 'eval_num_tokens': 552039.0, 'eval_mean_token_accuracy': 0.9300931276697101, 'epoch': 0.7692307692307693}\n",
      "{'loss': 0.2048, 'grad_norm': 0.3550415635108948, 'learning_rate': 0.00016280068724776797, 'epoch': 1.523076923076923}\n",
      "{'eval_loss': 0.3042583763599396, 'eval_runtime': 29.3496, 'eval_samples_per_second': 11.005, 'eval_steps_per_second': 1.124, 'eval_num_tokens': 1080546.0, 'eval_mean_token_accuracy': 0.9348858269778165, 'epoch': 1.523076923076923}\n",
      "{'loss': 0.2254, 'grad_norm': 0.4151623249053955, 'learning_rate': 0.00011714299149674537, 'epoch': 2.276923076923077}\n",
      "{'eval_loss': 0.2938467562198639, 'eval_runtime': 29.1559, 'eval_samples_per_second': 11.078, 'eval_steps_per_second': 1.132, 'eval_num_tokens': 1609561.0, 'eval_mean_token_accuracy': 0.9358767957398386, 'epoch': 2.276923076923077}\n",
      "{'loss': 0.1458, 'grad_norm': 0.42576169967651367, 'learning_rate': 6.717714596378137e-05, 'epoch': 3.0307692307692307}\n",
      "{'eval_loss': 0.2974730134010315, 'eval_runtime': 29.4702, 'eval_samples_per_second': 10.96, 'eval_steps_per_second': 1.12, 'eval_num_tokens': 2141411.0, 'eval_mean_token_accuracy': 0.9367139213012926, 'epoch': 3.0307692307692307}\n",
      "{'loss': 0.1118, 'grad_norm': 0.4354138970375061, 'learning_rate': 2.5459905785704042e-05, 'epoch': 3.8}\n",
      "{'eval_loss': 0.3245163559913635, 'eval_runtime': 29.4116, 'eval_samples_per_second': 10.982, 'eval_steps_per_second': 1.122, 'eval_num_tokens': 2689661.0, 'eval_mean_token_accuracy': 0.9325669913580923, 'epoch': 3.8}\n",
      "{'loss': 0.1124, 'grad_norm': 0.4652675986289978, 'learning_rate': 2.4750957494826033e-06, 'epoch': 4.553846153846154}\n",
      "{'eval_loss': 0.3132854700088501, 'eval_runtime': 29.3686, 'eval_samples_per_second': 10.998, 'eval_steps_per_second': 1.124, 'eval_num_tokens': 3219484.0, 'eval_mean_token_accuracy': 0.9359004678148212, 'epoch': 4.553846153846154}\n",
      "{'train_runtime': 2091.3079, 'train_samples_per_second': 3.089, 'train_steps_per_second': 0.077, 'train_loss': 0.1839571975171566, 'num_tokens': 3440007.0, 'mean_token_accuracy': 0.959784047890313, 'epoch': 4.861538461538462}\n",
      "trainable parameters: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mphi_2_rag_covid_k3_5e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/hf76a9qp\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250430_232204-hf76a9qp/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi_2_rag_covid_k3_5e_10bs\" \\\n",
    "  --num_epochs 5 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"covid\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 3 \\\n",
    "  --save_path \"../models/covid/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/covid/ft_vs_covid_150_20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fd1e3-7afd-4412-86f2-ae122e18991e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e60f9d-0565-4f82-b7d4-5f41204f38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model from LoRA adapter: ../models/covid/adapters/best_phi_2_rag_covid_k3_5e_10bs\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 18.40it/s]\n",
      "Load model embedding : ../models/covid/embedding/bge-small-covid_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/covid/ft_vs_covid_150_20\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Loading dataset splits for covid\n",
      "Train: 1292\n",
      "Val: 323\n",
      "Test: 404\n",
      "Datasets loaded and prepared.\n",
      "🔍 Buscando: 100%|████████████████████████████████| 9/9 [00:00<00:00, 18.75it/s]\n",
      "Generating prompts: 100%|███████████████████| 404/404 [00:00<00:00, 7700.66it/s]\n",
      "Processing inference\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --lora_adapter_path \"../models/covid/adapters/best_phi_2_rag_covid_k3_5e_10bs\" \\\n",
    "  --max_new_tokens 80 \\\n",
    "  --vector_store_path \"../vector_stores/covid/ft_vs_covid_150_20\" \\\n",
    "  --dataset_name \"covid\" \\\n",
    "  --output_csv_path \"../results/covid/full_ft_covid.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042108ba-2ae1-4097-a726-586e89b6c28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (env_2)",
   "language": "python",
   "name": "env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
