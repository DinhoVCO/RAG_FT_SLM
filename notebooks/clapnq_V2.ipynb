{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139c38c8-8865-439a-af75-9f3520fed08e",
   "metadata": {},
   "source": [
    "# ClapNQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b68689-7554-49d5-b7e5-13132ffcc2f6",
   "metadata": {},
   "source": [
    "## Create vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac6ad01-3256-4d7e-a79c-5c95f50e4865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "100%|████████████████████████████████| 178890/178890 [00:06<00:00, 28154.16it/s]\n",
      "  0%|                                     | 330/178890 [00:00<03:33, 836.17it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████| 178890/178890 [04:15<00:00, 699.53it/s]\n",
      "Total number of passages: 263742\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 261999\n",
      "Creating vector store\n",
      "Load model embedding : BAAI/bge-small-en-v1.5\n",
      "Using device: cuda\n",
      "Generando embeddings: 100%|███████████████████| 256/256 [03:45<00:00,  1.14it/s]\n",
      "✅ Índice FAISS creado exitosamente.\n",
      "💾 Vector store saved in ../vector_stores/clapnq/base_vs_clapnq_150_20\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"clapnq\" \\\n",
    "  --emb_model \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 1024 \\\n",
    "  --output_dir \"../vector_stores/clapnq/base_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ddfa907-15e1-4574-8d6c-ee13b0d634e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : BAAI/bge-small-en-v1.5\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/clapnq/base_vs_clapnq_150_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|████████████████████████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['Yellowstone (U.S. TV series), Principal photography for the series began in August 2017 at the Chief Joseph Ranch in Darby , Montana , which stands in as the home of John Dutton . Filming also took place that month near Park City , Utah . The production used all three soundstages at the Utah Film Studio in Park City , which is a total of 45,000 square feet . The building also houses offices , editing , a huge wardrobe department and construction shops . By November 2017 , the series had filmed in more than twenty locations in Utah , including the Salt Flats and Spanish Fork . Additionally , filming also took place at various locations in Montana . Production was reportedly set to last until December 2017 .',\n",
       "   'Yellowstone (U.S. TV series), In 2013 , Taylor Sheridan began work on the series , having recently grown tired of acting and begun writing screenplays . Having lived in the rural parts of states such as Texas and Wyoming , Sheridan purposely set the series in Montana and went about writing the first scripts in Livingston .'],\n",
       "  array([0.88373935, 0.7949742 ], dtype=float32))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/clapnq/base_vs_clapnq_150_20\")\n",
    "results = vector_store.buscar_por_batches(['where are they filming the tv series yellowstone?'],2)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf71a69-bd74-48b7-9b57-04ad88cf4d0d",
   "metadata": {},
   "source": [
    "## Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b39b26-4775-4324-a65f-433d7451d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset splits for clapnq\n",
      "Train: 2996\n",
      "Val: 749\n",
      "Test: 600\n",
      "Datasets loaded and prepared.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64140eb87bb24b70a85aac4fdb0a8e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2196f80b5da94b7299009e776e706fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5b66508131423c9be65e660d449ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9764cb8396054e368f5f42dfaa8aa0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03096287b7d4acba3a67adcd0b2e40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f383f3fd8be942da8715e0ed7fb95bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded and prepared.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_id': '-1253448975620540114',\n",
       " 'question': '\\u200ba budget prepared by the president and submitted to congress is called the',\n",
       " 'relevant_docs': \"United States budget process, The United States budget process begins when the President of the United States submits a budget request to Congress . The President 's budget is formulated over a period of months with the assistance of the Office of Management and Budget ( OMB ) , the largest office within the Executive Office of the President . The budget request includes funding requests for all federal executive departments and independent agencies . Budget documents include supporting documents and historical budget data and contains detailed information on spending and revenue proposals , along with policy proposals and initiatives with significant budgetary implications . The President 's budget request constitutes an extensive proposal of the administration 's intended revenue and spending plans for the following fiscal year . The budget proposal includes volumes of supporting information intended to persuade Congress of the necessity and value of the budget provisions . In addition , each federal executive department and independent agency provides additional detail and supporting documentation on its own funding requests . The documents are also posted on the OMB website . \"}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from utils.data_for_train_emb import load_and_prepare_datasets\n",
    "train_dataset, val_dataset, test_dataset = load_and_prepare_datasets('clapnq')\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6964965f-2990-4842-813d-eea54a0c2118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Starting main process...\n",
      "Loading dataset splits for clapnq\n",
      "Train: 2996\n",
      "Val: 749\n",
      "Test: 600\n",
      "Datasets loaded and prepared.\n",
      "Datasets loaded and prepared.\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/test_sbbd/notebooks2/wandb/run-20250503_031504-w422r8l5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbge-small-clapnq_10e_128bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/w422r8l5\u001b[0m\n",
      "Loading model: BAAI/bge-small-en-v1.5\n",
      "Model and loss function initialized.\n",
      "Configuring training arguments...\n",
      "Training arguments configured.\n",
      "Starting training process...\n",
      "{'loss': 0.327, 'grad_norm': 1.2697131633758545, 'learning_rate': 4.999098819177214e-05, 'epoch': 1.1538461538461537}\n",
      "{'eval_loss': 0.043905969709157944, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_recall@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9906564494575052, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9873417721518988, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9873417721518988, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9873417721518988, 'eval_telecom-ir-eval_cosine_map@100': 0.9873417721518988, 'eval_runtime': 1.1159, 'eval_samples_per_second': 353.964, 'eval_steps_per_second': 3.584, 'epoch': 1.1538461538461537}\n",
      "{'loss': 0.0527, 'grad_norm': 0.5458283424377441, 'learning_rate': 4.77281074950681e-05, 'epoch': 2.3076923076923075}\n",
      "{'eval_loss': 0.03181520476937294, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_recall@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9915908045117547, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_map@100': 0.9886075949367089, 'eval_runtime': 1.1179, 'eval_samples_per_second': 353.356, 'eval_steps_per_second': 3.578, 'epoch': 2.3076923076923075}\n",
      "{'loss': 0.0287, 'grad_norm': 0.38780030608177185, 'learning_rate': 4.182779518568926e-05, 'epoch': 3.4615384615384617}\n",
      "{'eval_loss': 0.026629813015460968, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_recall@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9915908045117546, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_map@100': 0.9886075949367089, 'eval_runtime': 1.1215, 'eval_samples_per_second': 352.219, 'eval_steps_per_second': 3.567, 'epoch': 3.4615384615384617}\n",
      "{'loss': 0.0191, 'grad_norm': 0.18044430017471313, 'learning_rate': 3.323434381080199e-05, 'epoch': 4.615384615384615}\n",
      "{'eval_loss': 0.026085354387760162, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_recall@1': 0.9772151898734177, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9915908045117546, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9886075949367089, 'eval_telecom-ir-eval_cosine_map@100': 0.9886075949367089, 'eval_runtime': 1.1105, 'eval_samples_per_second': 355.697, 'eval_steps_per_second': 3.602, 'epoch': 4.615384615384615}\n",
      "{'loss': 0.017, 'grad_norm': 0.39174535870552063, 'learning_rate': 2.3323058890224938e-05, 'epoch': 5.769230769230769}\n",
      "{'eval_loss': 0.026354029774665833, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_recall@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9906564494575052, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9873417721518988, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9873417721518988, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9873417721518988, 'eval_telecom-ir-eval_cosine_map@100': 0.9873417721518988, 'eval_runtime': 1.1066, 'eval_samples_per_second': 356.945, 'eval_steps_per_second': 3.615, 'epoch': 5.769230769230769}\n",
      "{'loss': 0.013, 'grad_norm': 0.398222953081131, 'learning_rate': 1.3680153490759073e-05, 'epoch': 6.923076923076923}\n",
      "{'eval_loss': 0.026572400704026222, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_recall@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9903249817269445, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_map@100': 0.9869198312236288, 'eval_runtime': 1.122, 'eval_samples_per_second': 352.051, 'eval_steps_per_second': 3.565, 'epoch': 6.923076923076923}\n",
      "{'loss': 0.0117, 'grad_norm': 0.46002885699272156, 'learning_rate': 5.848888922025553e-06, 'epoch': 8.076923076923077}\n",
      "{'eval_loss': 0.026596084237098694, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_recall@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9903249817269445, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_map@100': 0.9869198312236288, 'eval_runtime': 1.1183, 'eval_samples_per_second': 353.204, 'eval_steps_per_second': 3.577, 'epoch': 8.076923076923077}\n",
      "{'loss': 0.0154, 'grad_norm': 0.49055349826812744, 'learning_rate': 1.0825894793143721e-06, 'epoch': 9.23076923076923}\n",
      "{'eval_loss': 0.026463571935892105, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_accuracy@3': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_recall@1': 0.9746835443037974, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9903249817269445, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9869198312236287, 'eval_telecom-ir-eval_cosine_map@100': 0.9869198312236288, 'eval_runtime': 1.1132, 'eval_samples_per_second': 354.849, 'eval_steps_per_second': 3.593, 'epoch': 9.23076923076923}\n",
      "{'train_runtime': 78.1265, 'train_samples_per_second': 199.548, 'train_steps_per_second': 1.664, 'train_loss': 0.056753473385022235, 'epoch': 10.0}\n",
      "Training completed.\n",
      "Saving the trained model...\n",
      "Model saved successfully.\n",
      "Process completed successfully.\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mbge-small-clapnq_10e_128bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/w422r8l5\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250503_031504-w422r8l5/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/train_embedding.py \\\n",
    "  --name_dataset \"clapnq\" \\\n",
    "  --model_name \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --new_model_name \"bge-small-clapnq\" \\\n",
    "  --epochs 10 \\\n",
    "  --batch_size 128 \\\n",
    "  --output_dir \"../models/clapnq/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07508b1-0d1d-4bb5-bfd8-c0c6a60949a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0145c8bc-7e97-4517-8eba-955e7b097f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos...\n",
      "Models loaded\n",
      "Loading datasets...\n",
      "Loading dataset splits for clapnq\n",
      "Train: 2996\n",
      "Val: 749\n",
      "Test: 600\n",
      "Datasets loaded and prepared.\n",
      "Datasets loaded and prepared.\n",
      "Loaded dataset\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "Evaluating models\n",
      "Save results...\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/evaluate_embedding.py \\\n",
    "  --name_dataset \"clapnq\" \\\n",
    "  --output_dir \"../results/clapnq/\" \\\n",
    "  --models_dir \"../models/clapnq/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831fded-b089-456a-848b-4da1fd8880ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating new Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd969c8-cc50-4cb5-9885-876960f44b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "100%|████████████████████████████████| 178890/178890 [00:06<00:00, 29733.36it/s]\n",
      "  0%|                                     | 329/178890 [00:00<03:33, 835.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████| 178890/178890 [04:19<00:00, 690.14it/s]\n",
      "Total number of passages: 263742\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 261999\n",
      "Creating vector store\n",
      "Load model embedding : ../models/clapnq/embedding/bge-small-clapnq_10e_128bs\n",
      "Using device: cuda\n",
      "Generando embeddings: 100%|███████████████████| 256/256 [03:47<00:00,  1.12it/s]\n",
      "✅ Índice FAISS creado exitosamente.\n",
      "💾 Vector store saved in ../vector_stores/clapnq/ft_vs_clapnq_150_20\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"clapnq\" \\\n",
    "  --emb_model \"../models/clapnq/embedding/bge-small-clapnq_10e_128bs\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 1024 \\\n",
    "  --output_dir \"../vector_stores/clapnq/ft_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090613c1-a963-4d21-ba7a-169e101560f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : ../models/clapnq/embedding/bge-small-clapnq_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/clapnq/ft_vs_clapnq_150_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|████████████████████████████| 1/1 [00:00<00:00,  2.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['Yellowstone (U.S. TV series), Principal photography for the series began in August 2017 at the Chief Joseph Ranch in Darby , Montana , which stands in as the home of John Dutton . Filming also took place that month near Park City , Utah . The production used all three soundstages at the Utah Film Studio in Park City , which is a total of 45,000 square feet . The building also houses offices , editing , a huge wardrobe department and construction shops . By November 2017 , the series had filmed in more than twenty locations in Utah , including the Salt Flats and Spanish Fork . Additionally , filming also took place at various locations in Montana . Production was reportedly set to last until December 2017 .',\n",
       "   'Yellowstone (U.S. TV series), In 2013 , Taylor Sheridan began work on the series , having recently grown tired of acting and begun writing screenplays . Having lived in the rural parts of states such as Texas and Wyoming , Sheridan purposely set the series in Montana and went about writing the first scripts in Livingston .'],\n",
       "  array([0.939727  , 0.90734386], dtype=float32))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/clapnq/ft_vs_clapnq_150_20\")\n",
    "results = vector_store.buscar_por_batches(['where are they filming the tv series yellowstone?'],2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba1b662-8924-4218-8a07-44dc275d2ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : ../models/clapnq/embedding/bge-small-clapnq_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/clapnq/ft_vs_clapnq_150_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|████████████████████████████| 1/1 [00:00<00:00,  2.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['Yellowstone (U.S. TV series), Principal photography for the series began in August 2017 at the Chief Joseph Ranch in Darby , Montana , which stands in as the home of John Dutton . Filming also took place that month near Park City , Utah . The production used all three soundstages at the Utah Film Studio in Park City , which is a total of 45,000 square feet . The building also houses offices , editing , a huge wardrobe department and construction shops . By November 2017 , the series had filmed in more than twenty locations in Utah , including the Salt Flats and Spanish Fork . Additionally , filming also took place at various locations in Montana . Production was reportedly set to last until December 2017 .',\n",
       "   'Yellowstone (U.S. TV series), In 2013 , Taylor Sheridan began work on the series , having recently grown tired of acting and begun writing screenplays . Having lived in the rural parts of states such as Texas and Wyoming , Sheridan purposely set the series in Montana and went about writing the first scripts in Livingston .'],\n",
       "  array([0.9226119, 0.8790455], dtype=float32))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/clapnq/ft_vs_clapnq_150_20\")\n",
    "results = vector_store.buscar_por_batches(['where are they filming the tv series yellowstone?'],2)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec2237-5382-498a-923a-29e8d124ffc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2137212-9218-44c4-89c9-d759c65e7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_for_train_phi import get_dataset_for_train_phi\n",
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/clapnq/ft_vs_clapnq_150_20\")\n",
    "train_ds, test_ds = get_dataset_for_train_phi('clapnq', True, vector_store,4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef47bd5-77b2-4971-81db-3e3e9d4a5828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Using the information in the context, answer the question as concisely and faithfully as possible. If the context does not contain enough information,  respond with unanswerable.\n",
      "Context:\n",
      "\n",
      "Document 0:United States budget process, The United States budget process begins when the President of the United States submits a budget request to Congress . The President 's budget is formulated over a period of months with the assistance of the Office of Management and Budget ( OMB ) , the largest office within the Executive Office of the President . The budget request includes funding requests for all federal executive departments and independent agencies . Budget documents include supporting documents and historical budget data and contains detailed information on spending and revenue proposals , along with policy proposals and initiatives with significant budgetary implications . The President 's budget request constitutes an extensive proposal of the administration 's intended revenue and spending plans for the following fiscal year\n",
      "Document 1:. The budget proposal includes volumes of supporting information intended to persuade Congress of the necessity and value of the budget provisions . In addition , each federal executive department and independent agency provides additional detail and supporting documentation on its own funding requests . The documents are also posted on the OMB website .\n",
      "Document 2:United States budget process, The Budget and Accounting Act of 1921 requires the President to submit the budget to Congress for each fiscal year , which is the 12 - month period beginning on October 1 and ending on September 30 of the next calendar year . The current federal budget law ( 31 U.S.C. § 1105 ( a ) ) requires that the President submit the budget between the first Monday in January and the first Monday in February . In recent times , the President 's budget submission has been issued in the first week of February . The budget submission has been delayed , however , in some new presidents ' first year when the previous president belonged to a different party\n",
      "Document 3:United States budget process, Prior to 1974 , Congress had no formal process for establishing a federal budget . When President Richard Nixon began to refuse to spend funds that Congress had allocated , they adopted a more formal means by which to challenge him . The Congressional Budget Act of 1974 created the Congressional Budget Office ( CBO ) , which gained more control of the budget , limiting the power of the President 's Office of Management and Budget ( OMB ) . The Act passed easily while the administration was embroiled in the Watergate scandal and was unwilling to provoke Congress .\n",
      "Document 4:United States budget process, The United States budget process begins when the President of the United States submits a budget request to Congress . The President 's budget is formulated over a period of months with the assistance of the Office of Management and Budget ( OMB ) , the largest office within the Executive Office of the President . The budget request includes funding requests for all federal executive departments and independent agencies . Budget documents include supporting documents and historical budget data and contains detailed information on spending and revenue proposals , along with policy proposals and initiatives with significant budgetary implications . The President 's budget request constitutes an extensive proposal of the administration 's intended revenue and spending plans for the following fiscal year\n",
      "Document 5:. The budget proposal includes volumes of supporting information intended to persuade Congress of the necessity and value of the budget provisions . In addition , each federal executive department and independent agency provides additional detail and supporting documentation on its own funding requests . The documents are also posted on the OMB website .\n",
      "\n",
      "Question:\n",
      "​a budget prepared by the president and submitted to congress is called the\n",
      "\n",
      "Output:\n",
      "It is called the President's budget request.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be29613-3cd1-465c-a627-0e879243190c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/test_sbbd/notebooks2/wandb/run-20250503_123126-uxfwni31\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi_2_rag_k1_clapnq-2e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/uxfwni31\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "Load model embedding : ../models/clapnq/embedding/bge-small-clapnq_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/clapnq/ft_vs_clapnq_150_20\n",
      "Using k = 1 passages\n",
      "Creating dataset for clapnq\n",
      "Loading dataset splits for clapnq\n",
      "Train: 2996\n",
      "Val: 749\n",
      "Test: 600\n",
      "Datasets loaded and prepared.\n",
      "🔍 Buscando: 100%|████████████████████████████| 375/375 [00:17<00:00, 20.85it/s]\n",
      "🔍 Buscando: 100%|██████████████████████████████| 94/94 [00:04<00:00, 21.12it/s]\n",
      "Input Example:\n",
      "Instruct: Using the information in the context, answer the question as concisely and faithfully as possible. If the context does not contain enough information,  respond with unanswerable.\n",
      "Context:\n",
      "\n",
      "Document 0:United States budget process, The United States budget process begins when the President of the United States submits a budget request to Congress . The President 's budget is formulated over a period of months with the assistance of the Office of Management and Budget ( OMB ) , the largest office within the Executive Office of the President . The budget request includes funding requests for all federal executive departments and independent agencies . Budget documents include supporting documents and historical budget data and contains detailed information on spending and revenue proposals , along with policy proposals and initiatives with significant budgetary implications . The President 's budget request constitutes an extensive proposal of the administration 's intended revenue and spending plans for the following fiscal year\n",
      "Document 1:United States budget process, The United States budget process begins when the President of the United States submits a budget request to Congress . The President 's budget is formulated over a period of months with the assistance of the Office of Management and Budget ( OMB ) , the largest office within the Executive Office of the President . The budget request includes funding requests for all federal executive departments and independent agencies . Budget documents include supporting documents and historical budget data and contains detailed information on spending and revenue proposals , along with policy proposals and initiatives with significant budgetary implications . The President 's budget request constitutes an extensive proposal of the administration 's intended revenue and spending plans for the following fiscal year\n",
      "Document 2:. The budget proposal includes volumes of supporting information intended to persuade Congress of the necessity and value of the budget provisions . In addition , each federal executive department and independent agency provides additional detail and supporting documentation on its own funding requests . The documents are also posted on the OMB website .\n",
      "\n",
      "Question:\n",
      "​a budget prepared by the president and submitted to congress is called the\n",
      "\n",
      "Output:\n",
      "It is called the President's budget request.\n",
      "<|endoftext|>\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "Training\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "🟢 No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 2.0097, 'grad_norm': 0.20378650724887848, 'learning_rate': 0.00019164590810602262, 'epoch': 0.3333333333333333}\n",
      "{'eval_loss': 1.7344483137130737, 'eval_runtime': 52.6557, 'eval_samples_per_second': 14.224, 'eval_steps_per_second': 1.424, 'eval_num_tokens': 446833.0, 'eval_mean_token_accuracy': 0.63469455798467, 'epoch': 0.3333333333333333}\n",
      "{'loss': 1.7432, 'grad_norm': 0.18406276404857635, 'learning_rate': 0.0001578986789811849, 'epoch': 0.6666666666666666}\n",
      "{'eval_loss': 1.6633092164993286, 'eval_runtime': 52.5271, 'eval_samples_per_second': 14.259, 'eval_steps_per_second': 1.428, 'eval_num_tokens': 857552.0, 'eval_mean_token_accuracy': 0.6471327336629232, 'epoch': 0.6666666666666666}\n",
      "{'loss': 1.707, 'grad_norm': 0.20244933664798737, 'learning_rate': 0.00010757588904528106, 'epoch': 1.0}\n",
      "{'eval_loss': 1.6443895101547241, 'eval_runtime': 52.6694, 'eval_samples_per_second': 14.221, 'eval_steps_per_second': 1.424, 'eval_num_tokens': 1250441.0, 'eval_mean_token_accuracy': 0.6494770193099976, 'epoch': 1.0}\n",
      "{'loss': 1.6886, 'grad_norm': 0.17408670485019684, 'learning_rate': 5.5084230807412126e-05, 'epoch': 1.3333333333333333}\n",
      "{'eval_loss': 1.6337496042251587, 'eval_runtime': 52.7199, 'eval_samples_per_second': 14.207, 'eval_steps_per_second': 1.423, 'eval_num_tokens': 1695835.0, 'eval_mean_token_accuracy': 0.6517642331123352, 'epoch': 1.3333333333333333}\n",
      "{'loss': 1.6614, 'grad_norm': 0.18337368965148926, 'learning_rate': 1.5451312643206827e-05, 'epoch': 1.6666666666666665}\n",
      "{'eval_loss': 1.6296370029449463, 'eval_runtime': 52.4467, 'eval_samples_per_second': 14.281, 'eval_steps_per_second': 1.43, 'eval_num_tokens': 2107245.0, 'eval_mean_token_accuracy': 0.6526494208971659, 'epoch': 1.6666666666666665}\n",
      "{'loss': 1.6589, 'grad_norm': 0.21233662962913513, 'learning_rate': 2.347019815158724e-08, 'epoch': 2.0}\n",
      "{'eval_loss': 1.6292022466659546, 'eval_runtime': 52.511, 'eval_samples_per_second': 14.264, 'eval_steps_per_second': 1.428, 'eval_num_tokens': 2500882.0, 'eval_mean_token_accuracy': 0.6527118754386901, 'epoch': 2.0}\n",
      "{'train_runtime': 1698.5549, 'train_samples_per_second': 3.528, 'train_steps_per_second': 0.088, 'train_loss': 1.7447997283935548, 'num_tokens': 2500882.0, 'mean_token_accuracy': 0.6322996378938357, 'epoch': 2.0}\n",
      "trainable parameters: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mphi_2_rag_k1_clapnq-2e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/uxfwni31\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250503_123126-uxfwni31/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi_2_rag_k1_clapnq-5e_10bs\" \\\n",
    "  --num_epochs 5 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"clapnq\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 1 \\\n",
    "  --save_path \"../models/clapnq/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/clapnq/ft_vs_clapnq_150_20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036721cc-e913-4c85-8e88-13a0d07f3bf6",
   "metadata": {},
   "source": [
    "## inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399ea629-5df9-47e1-a50b-bbf69761f22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model from LoRA adapter: ../models/clapnq/adapters/best_phi_2_rag_k1_clapnq-5e_10bs\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00, 18.78it/s]\n",
      "Load model embedding : ../models/clapnq/embedding/bge-small-clapnq_10e_128bs\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/clapnq/ft_vs_clapnq_150_20\n",
      "Device set to use cuda:0\n",
      "Loading dataset splits for clapnq\n",
      "Train: 2996\n",
      "Val: 749\n",
      "Test: 600\n",
      "Datasets loaded and prepared.\n",
      "Dataset({\n",
      "    features: ['id', 'question', 'passages', 'output'],\n",
      "    num_rows: 600\n",
      "})\n",
      "🔍 Buscando: 100%|██████████████████████████████| 12/12 [00:01<00:00,  7.10it/s]\n",
      "Generating prompts: 100%|███████████████████| 600/600 [00:00<00:00, 8176.78it/s]\n",
      "Example prompt\n",
      "Instruct: Using the information in the context, answer the question as concisely and faithfully as possible. If the context does not contain enough information,  respond with unanswerable.\n",
      "Context:\n",
      "\n",
      "Document 1:Forecasting, Forecasting has applications in a wide range of fields where estimates of future conditions are useful . Not everything can be forecasted reliably , if the factors that relate to what is being forecast are known and well understood and there is a significant amount of data that can be used very reliable forecasts can often be obtained . If this is not the case or if the actual outcome is effected by the forecasts , the reliability of the forecasts can be significantly lower .\n",
      "Document 2:Forecasting, As proposed by Edward Lorenz in 1963 , long range weather forecasts , those made at a range of two weeks or more , are impossible to definitively predict the state of the atmosphere , owing to the chaotic nature of the fluid dynamics equations involved . Extremely small errors in the initial input , such as temperatures and winds , within numerical models double every five days .\n",
      "Document 3:Forecasting, A variation on the naïve method is to allow the forecasts to increase or decrease over time , where the amount of change over time ( called the drift ) is set to be the average change seen in the historical data . So the forecast for time T + h ( \\ displaystyle T + h ) is given by\n",
      "\n",
      "Question:\n",
      "which method of forecasting uses averages to predict future weather\n",
      "\n",
      "Output:\n",
      "Processing inference\n",
      "inference completed\n",
      "answer save in ../results/clapnq/full_ft_clapnq_k3_5e.csv\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --lora_adapter_path \"../models/clapnq/adapters/best_phi_2_rag_k1_clapnq-5e_10bs\" \\\n",
    "  --max_new_tokens 150 \\\n",
    "  --vector_store_path \"../vector_stores/clapnq/ft_vs_clapnq_150_20\" \\\n",
    "  --dataset_name \"clapnq\" \\\n",
    "  --output_csv_path \"../results/clapnq/full_ft_clapnq_k3.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 8 \\\n",
    "  --top_k 3 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf9da9-9c07-4580-97b8-6449c218305c",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9934a3-b6c6-47dc-ae6b-a808c9b3e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset splits for clapnq\n",
      "Train: 2996\n",
      "Val: 749\n",
      "Test: 600\n",
      "Datasets loaded and prepared.\n",
      "Results saved in: ../results/clapnq/full_ft_clapnq_k3.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluate_inference import evaluate_answer\n",
    "evaluate_answer('clapnq', '../results/clapnq/full_ft_clapnq_k3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742134e-dae9-4028-8d65-d371fea96f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (env_sbbd)",
   "language": "python",
   "name": "env_sbbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
