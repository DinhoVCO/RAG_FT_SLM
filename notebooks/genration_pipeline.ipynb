{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b5634e-4667-43af-87ec-c093c9d45fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9579d6d1-28bf-45a1-9b0b-2ae2f1df512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Instruct:  Using the information in the context, answer the question as concisely and faithfully as possible.If the context does not provide enough information to answer confidently, answer based on the most likely interpretation from the given text.\n",
    "\n",
    "Context:\n",
    "\n",
    "Document 0:. As of 26 January 2020, the still ongoing outbreak had resulted in 2066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (WHO) news release via https://www.who.int/csr/don/en/ from 14 to 21 January 2020\n",
    "Document 1:The first three cases detected were reported in France on 24 January 2020 and had onset of symptoms on 17, 19 and 23 January respectively [10] . The first death was reported on 15 February in France. As at 21 February, nine countries had reported cases ( Figure) : Belgium (1), Finland (1), France (12), Germany (16), Italy (3), Russia (2), Spain (2), Sweden (1) and the UK (9 -not included further).\n",
    "Document 2:. Nonetheless, the virus spread from Wuhan to other cities in China and outside the country. By February 4, 2020, a total of 23 locations outside mainland China reported cases, 22 of which reported imported cases; Spain reported a case caused by secondary transmission (3).\n",
    "Document 3:066 (618 of them are in Wuhan) confirmed cases and 56 (45 of them were in Wuhan) deaths in mainland China [4] , and sporadic cases exported from Wuhan were reported in Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (W\n",
    "Document 4:As at 09:00 on 21 February, few COVID-19 cases had been detected in Europe compared with Asia. However the situation is rapidly developing, with a large outbreak recently identified in northern Italy, with transmission in several municipalities and at least two deaths [18] . As at 5 March 2020, there are 4,250 cases including 113 deaths reported among 38 countries in the WHO European region [19] .\n",
    "\n",
    "Question:\n",
    "As of 26 January 2020, what countries had sporadic cases?\n",
    "\n",
    "Output:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40d025-2c2d-437f-beeb-cf563d5fa297",
   "metadata": {},
   "source": [
    "## phi original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979b711c-defb-421c-9059-5cdd77f223d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9a9221f5d74048849b7beba14ac229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072f2ae0-e6cf-43dc-9d58-3c447d5ba544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As of 26 January 2020, Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States had sporadic cases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=120,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    length_penalty=0.1,\n",
    "    do_sample=False,  # Desactiva sampling, activa búsqueda determinista\n",
    "    return_full_text=False \n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f209db0b-24e6-430a-919d-62d865a5265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from peft import PeftModel, PeftConfig\n",
    "def load_model_auto(device=\"cuda\", model_name=None, lora_adapter_path=None):\n",
    "    if lora_adapter_path:\n",
    "        print(f\"Loading base model from LoRA adapter: {lora_adapter_path}\")\n",
    "        config = PeftConfig.from_pretrained(lora_adapter_path)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        ).to(device)\n",
    "        model = PeftModel.from_pretrained(model, lora_adapter_path).to(device)\n",
    "        model = model.merge_and_unload()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, padding_side=\"left\")\n",
    "    elif model_name:\n",
    "        print(f\"Loading base model: {model_name}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        ).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "    else:\n",
    "        raise ValueError(\"You must provide either a model_name or a lora_adapter_path.\")\n",
    "\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    # if tokenizer.pad_token_id is None:\n",
    "    #     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    \n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364bbda1-07da-44ad-9eca-64edc8afb30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from LoRA adapter: ../models/covid/adapters/phi_2_rag_covid_k3_5e_10bs/checkpoint-160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae14208323d4433ad23336734e11a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model, tokenizer = load_model_auto('cuda', None, \"../models/covid/adapters/best_phi_2_rag_covid_k3_5e_10bs\")\n",
    "model, tokenizer = load_model_auto('cuda', None, \"../models/covid/adapters/phi_2_rag_covid_k3_5e_10bs/checkpoint-160\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e7c260e-d198-4516-81e6-3a95164a90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator2 = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb81b7a-7996-407c-9a99-8fbc4a5532a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wuhan, Thailand, Japan, Republic of Korea, Hong Kong, Taiwan, Australia, and the United States, please see the World Health Organization (WHO) news release via https://www.who.int/csr/don/en/ from 14 to 21 January 2020. The first death was reported on 15 February in France. As at 21 February, nine countries had reported cases ( Figure) : Belgium (1), Finland (1), France (12), Germany (16), Italy (3), Russia (2), Spain (2), Sweden (1) and the UK (\n"
     ]
    }
   ],
   "source": [
    "output2 = generator2(\n",
    "    prompt,\n",
    "    max_new_tokens=120,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    length_penalty=1,\n",
    "    do_sample=False,  # Desactiva sampling, activa búsqueda determinista\n",
    "    return_full_text=False \n",
    ")\n",
    "\n",
    "print(output2[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031f6301-a1bf-4868-a8ca-31ee6f010adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['Â', '¿', 'Cu', 'Ã¡', 'l', 'Ġes', 'Ġla', 'Ġcapital', 'Ġde', 'ĠFranc', 'ia', '?']\n",
      "\n",
      "Token IDs:\n",
      "[126, 123, 46141, 6557, 75, 1658, 8591, 3139, 390, 4682, 544, 30]\n",
      "\n",
      "Tokens y sus IDs:\n",
      "'Â'                  -> 126\n",
      "'¿'                  -> 123\n",
      "'Cu'                 -> 46141\n",
      "'Ã¡'                 -> 6557\n",
      "'l'                  -> 75\n",
      "'Ġes'                -> 1658\n",
      "'Ġla'                -> 8591\n",
      "'Ġcapital'           -> 3139\n",
      "'Ġde'                -> 390\n",
      "'ĠFranc'             -> 4682\n",
      "'ia'                 -> 544\n",
      "'?'                  -> 30\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizer de Phi-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"¿Cuál es la capital de Francia?\"\n",
    "\n",
    "# Tokenizar\n",
    "tokens = tokenizer.tokenize(text)\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "# Mostrar tokens y sus IDs\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nToken IDs:\")\n",
    "print(input_ids)\n",
    "\n",
    "# Mostrar tokens junto a sus IDs\n",
    "print(\"\\nTokens y sus IDs:\")\n",
    "for token, token_id in zip(tokens, input_ids):\n",
    "    print(f\"{token!r:20} -> {token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5569d01b-4e5e-4319-be40-10e030698743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto con token final: ¿Cuál es la capital de Francia?<|endoftext|>\n",
      "Token IDs: [126, 123, 46141, 6557, 75, 1658, 8591, 3139, 390, 4682, 544, 30, 50256]\n",
      "Tokens: ['�', '�', 'Cu', 'á', 'l', ' es', ' la', ' capital', ' de', ' Franc', 'ia', '?', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "# Obtener el token de fin de texto\n",
    "eot_token = tokenizer.eos_token  # Esto debería ser '<|endoftext|>'\n",
    "eot_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Texto original\n",
    "text = \"¿Cuál es la capital de Francia?\"\n",
    "\n",
    "# Agregar manualmente el token de inicio y fin (en este caso, solo EOS si no hay BOS definido)\n",
    "text_with_tokens = text + eot_token\n",
    "\n",
    "# Tokenizar\n",
    "input_ids = tokenizer.encode(text_with_tokens, add_special_tokens=True)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\"Texto con token final:\", text_with_tokens)\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Tokens:\", [tokenizer.decode([tid]) for tid in input_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b9da384-3e14-4fbc-b2a8-2551d0f58719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [126, 123, 46141, 6557, 75, 1658, 8591, 3139, 390, 4682, 544, 30]\n",
      "Tokens: ['�', '�', 'Cu', 'á', 'l', ' es', ' la', ' capital', ' de', ' Franc', 'ia', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\",\n",
    "                                          add_eos_token=True,\n",
    "                                          trust_remote_code=True)\n",
    "text = \"¿Cuál es la capital de Francia?\"\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = \"left\"\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Tokens:\", [tokenizer.decode([tid]) for tid in input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bceff2d5-20fe-48fd-870b-ef48989c3fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b69f6d2-3977-4d8d-b08e-d9325d8bea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [126, 123, 46141, 6557, 75, 1658, 8591, 3139, 390, 4682, 544, 30]\n",
      "Tokens: ['�', '�', 'Cu', 'á', 'l', ' es', ' la', ' capital', ' de', ' Franc', 'ia', '?']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Tokens:\", [tokenizer.decode([tid]) for tid in input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1944b25b-1eaf-449d-89de-7360d21496e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>']\n",
      "tensor([[22362,    78,  1658,   555,   304,    73, 18856,    78,    13,   220,\n",
      "         50256],\n",
      "        [   36,  4169,  1658,   267, 23528,    13, 50256, 50256, 50256, 50256,\n",
      "         50256]])\n",
      "Esto es un ejemplo. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizer\n",
    "model_name = \"microsoft/phi-2\"  # O cualquier otro modelo que estés usando\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.all_special_tokens)\n",
    "# Secuencias de ejemplo\n",
    "sequences = [\"Esto es un ejemplo. <|endoftext|>\", \"Este es otro.\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# Tokenizar con padding\n",
    "tokenized = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Mostrar los tokens\n",
    "print(tokenized['input_ids'])\n",
    "# Convertir los IDs a tokens legibles\n",
    "tokens = tokenizer.decode(tokenized['input_ids'][0])  # Primer ejemplo\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e92168a-24af-4197-9213-33860f9ab990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Este es otro.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.decode(tokenized['input_ids'][1])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7405c-3de0-4a1d-989b-4470338cb5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (env_2)",
   "language": "python",
   "name": "env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
