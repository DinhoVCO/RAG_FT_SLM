{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ca0ed0-c01d-402b-9bc0-d1578c646e75",
   "metadata": {},
   "source": [
    "# TeleQnA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22152b-ca99-4688-a5f0-6e3be6a4c4cd",
   "metadata": {},
   "source": [
    "## Create vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553a16dc-4d7c-4a96-b3b7-9b460a1e0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "Total number of documents: 554\n",
      "Creating passages\n",
      "  5%|██▎                                       | 30/554 [00:27<08:40,  1.01it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████| 554/554 [10:41<00:00,  1.16s/it]\n",
      "Number of documents: 389606\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 378571\n",
      "Creating vector store\n",
      "Load model embedding : BAAI/bge-small-en-v1.5\n",
      "Using device: cuda\n",
      "Generando embeddings: 100%|███████████████████| 185/185 [05:32<00:00,  1.80s/it]\n",
      "✅ Índice FAISS creado exitosamente.\n",
      "💾 Vector store saved in ../vector_stores/teleqna/base_vs_teleqna_150_20\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"teleqna\" \\\n",
    "  --emb_model \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 2048 \\\n",
    "  --output_dir \"../vector_stores/teleqna/base_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2982078-9bc8-4904-8654-ec6bb9db8b4b",
   "metadata": {},
   "source": [
    "## Training embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2420ab28-8ec2-4061-959c-48506a268295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Starting main process...\n",
      "Loading dataset splits for teleqna\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Datasets loaded and prepared.\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/test_sbbd/notebooks2/wandb/run-20250503_163419-5856s1rq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbge-small-3gpp_10e_128bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/5856s1rq\u001b[0m\n",
      "Loading model: BAAI/bge-small-en-v1.5\n",
      "Model and loss function initialized.\n",
      "Configuring training arguments...\n",
      "Training arguments configured.\n",
      "Starting training process...\n",
      "{'loss': 0.3891, 'grad_norm': 1.2274510860443115, 'learning_rate': 4.734081600808531e-05, 'epoch': 2.5}\n",
      "{'eval_loss': 0.06870950758457184, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.9834254143646409, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 1.0, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.9834254143646409, 'eval_telecom-ir-eval_cosine_recall@1': 0.9834254143646409, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9927764423492613, 'eval_telecom-ir-eval_cosine_mrr@3': 0.988950276243094, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9903314917127072, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9903314917127072, 'eval_telecom-ir-eval_cosine_map@100': 0.9903314917127072, 'eval_runtime': 0.2941, 'eval_samples_per_second': 615.434, 'eval_steps_per_second': 6.8, 'epoch': 2.5}\n",
      "{'loss': 0.0459, 'grad_norm': 0.2939002513885498, 'learning_rate': 3.076539676856101e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.052123747766017914, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_recall@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9944040714954667, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9926335174953961, 'eval_telecom-ir-eval_cosine_map@100': 0.9926335174953959, 'eval_runtime': 0.2992, 'eval_samples_per_second': 605.019, 'eval_steps_per_second': 6.685, 'epoch': 5.0}\n",
      "{'loss': 0.0246, 'grad_norm': 0.2363736778497696, 'learning_rate': 1.0071035207430352e-05, 'epoch': 7.5}\n",
      "{'eval_loss': 0.05150328204035759, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_recall@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9944040714954667, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9926335174953961, 'eval_telecom-ir-eval_cosine_map@100': 0.9926335174953959, 'eval_runtime': 0.2966, 'eval_samples_per_second': 610.324, 'eval_steps_per_second': 6.744, 'epoch': 7.5}\n",
      "{'loss': 0.0158, 'grad_norm': 0.1372489035129547, 'learning_rate': 4.229604321829561e-08, 'epoch': 10.0}\n",
      "{'eval_loss': 0.052012063562870026, 'eval_telecom-ir-eval_cosine_accuracy@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_accuracy@3': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@5': 0.994475138121547, 'eval_telecom-ir-eval_cosine_accuracy@10': 1.0, 'eval_telecom-ir-eval_cosine_precision@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_recall@1': 0.988950276243094, 'eval_telecom-ir-eval_cosine_ndcg@10': 0.9944040714954667, 'eval_telecom-ir-eval_cosine_mrr@3': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@5': 0.9917127071823204, 'eval_telecom-ir-eval_cosine_mrr@10': 0.9926335174953961, 'eval_telecom-ir-eval_cosine_map@100': 0.9926335174953959, 'eval_runtime': 0.2989, 'eval_samples_per_second': 605.455, 'eval_steps_per_second': 6.69, 'epoch': 10.0}\n",
      "{'train_runtime': 10.1218, 'train_samples_per_second': 715.291, 'train_steps_per_second': 5.928, 'train_loss': 0.11883130272229513, 'epoch': 10.0}\n",
      "Training completed.\n",
      "Saving the trained model...\n",
      "Model saved successfully.\n",
      "Process completed successfully.\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mbge-small-3gpp_10e_128bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_embeddings/runs/5856s1rq\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250503_163419-5856s1rq/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/train_embedding.py \\\n",
    "  --name_dataset \"teleqna\" \\\n",
    "  --model_name \"BAAI/bge-small-en-v1.5\" \\\n",
    "  --new_model_name \"bge-small-3gpp\" \\\n",
    "  --epochs 10 \\\n",
    "  --batch_size 128 \\\n",
    "  --output_dir \"../models/teleqna/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef126dff-ef3a-47b5-b9c2-ee66307c70b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1865da05-66b4-4a94-a9c4-b4d7e3504258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelos...\n",
      "Models loaded\n",
      "Loading datasets...\n",
      "Loading dataset splits for teleqna\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Datasets loaded and prepared.\n",
      "Loaded dataset\n",
      "Creating evaluator...\n",
      "Evaluator created.\n",
      "Evaluating models\n",
      "Save results...\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/evaluate_embedding.py \\\n",
    "  --name_dataset \"teleqna\" \\\n",
    "  --output_dir \"../results/teleqna/\" \\\n",
    "  --models_dir \"../models/teleqna/embedding/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c923fc-55f2-448a-bedc-e85d52b14277",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create new vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be730a07-012a-48ea-ad16-1bc697ff3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding\n",
      "Using device: cuda\n",
      "Total number of documents: 554\n",
      "Creating passages\n",
      "  5%|██▎                                       | 30/554 [00:27<08:12,  1.06it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████| 554/554 [10:44<00:00,  1.16s/it]\n",
      "Number of documents: 389606\n",
      "Removing duplicate passages\n",
      "Total number of passages created: 378571\n",
      "Creating vector store\n",
      "Load model embedding : ../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\n",
      "Using device: cuda\n",
      "Generando embeddings: 100%|███████████████████| 185/185 [05:36<00:00,  1.82s/it]\n",
      "✅ Índice FAISS creado exitosamente.\n",
      "💾 Vector store saved in ../vector_stores/teleqna/ft_vs_teleqna_150_20\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_vector_store.py \\\n",
    "  --dataset \"teleqna\" \\\n",
    "  --emb_model \"../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\" \\\n",
    "  --cs 150 \\\n",
    "  --co 20 \\\n",
    "  --bs_emb 2048 \\\n",
    "  --output_dir \"../vector_stores/teleqna/ft_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e49cc-afb5-46b6-a3de-338bc748c126",
   "metadata": {},
   "source": [
    "## Train Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27383871-af53-45b6-aefe-c771e28c2084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model embedding : ../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/teleqna/ft_vs_teleqna_150_20\n",
      "Creating dataset for teleqna\n",
      "Loading dataset splits for teleqna\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Buscando: 100%|██████████████████████████| 91/91 [00:05<00:00, 15.52it/s]\n",
      "🔍 Buscando: 100%|██████████████████████████| 23/23 [00:01<00:00, 15.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b37bdeab6a41239a9ebf04f62ab150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from utils.data_for_train_phi import get_dataset_for_train_phi\n",
    "from vector_stores.faiss import VectorStoreFaiss\n",
    "vector_store = VectorStoreFaiss.load_local(\"../vector_stores/teleqna/ft_vs_teleqna_150_20\")\n",
    "train_ds, test_ds = get_dataset_for_train_phi('teleqna', True, vector_store,3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac3fcd3-3482-41c5-9e81-86877863f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Use the context provided below to answer the question. Select the correct option from A, B, C, D, E. Respond only with the the correct option. Do not include any explanations.\n",
      "Context:\n",
      "\n",
      "Document 0:1>\telse if the failure is detected due to Mobility from NR failure as described in 5.4.3.5, set the fields in VarRLF-report as follows:\n",
      "\n",
      "2>\tset the connectionFailureType to hof;\n",
      "\n",
      "2>\tif last MobilityFromNRCommand concerned a failed inter-RAT handover from NR to E-UTRA and if the UE supports Radio Link Failure Report for Inter-RAT MRO EUTRA (NR to EUTRA):\n",
      "Document 1:Release due to CN-detected mobility\n",
      "\n",
      "The context release is requested by the AMF because the UE is already served by another CN node (same or different system), or another NG interface of the same CN node.\n",
      "\n",
      "N26 interface not available\n",
      "\n",
      "The action failed due to a temporary failure of the N26 interface.\n",
      "\n",
      "Release due to pre-emption\n",
      "\n",
      "Release is initiated due to pre-emption.\n",
      "\n",
      "Multiple Location Reporting Reference ID Instances\n",
      "\n",
      "The action failed because multiple areas of interest are set with the same Location Reporting Reference ID.\n",
      "\n",
      "RSN not available for the UP\n",
      "\n",
      "The redundant user plane resources indicated by RSN are not available.\n",
      "\n",
      "NPN access denied\n",
      "\n",
      "Access was denied, or release is requested, for NPN reasons.\n",
      "\n",
      "CAG only access denied\n",
      "Document 2:\"Relocation failure\" is used by the target MME/S4-SGSN to indicate the source MME/S4-SGSN that the relocation has failed.\n",
      "Document 3:When the UE fails the mobility from NR procedure, it indicates the release of the RRC connection to upper layers together with the release cause 'RRC connection failure'.\n",
      "\n",
      "Question:\n",
      "What release cause is indicated when the UE fails the mobility from NR procedure?\n",
      "\n",
      "Options:\n",
      "A) 'Handover failure'\n",
      "B) 'RRC connection release'\n",
      "C) 'RRC connection failure'\n",
      "D) 'Mobility failure'\n",
      "E) 'Other'\n",
      "\n",
      "Output:\n",
      "C) 'RRC connection failure'\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb71b63b-c2f9-4540-a839-be594027af12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinho15971\u001b[0m (\u001b[33mdinho15971-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/local1/ronaldinho/projects/test_sbbd/notebooks2/wandb/run-20250503_171222-0a2q7ld0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mphi-2-rag-k3_teleqna-5e_10bs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/0a2q7ld0\u001b[0m\n",
      "Usando dispositivo: cuda\n",
      "Load model embedding : ../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/teleqna/ft_vs_teleqna_150_20\n",
      "Using k = 3 passages\n",
      "Creating dataset for teleqna\n",
      "Loading dataset splits for teleqna\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "🔍 Buscando: 100%|██████████████████████████████| 91/91 [00:06<00:00, 14.23it/s]\n",
      "🔍 Buscando: 100%|██████████████████████████████| 23/23 [00:01<00:00, 14.97it/s]\n",
      "Input Example:\n",
      "Instruct: Use the context provided below to answer the question. Select the correct option from A, B, C, D, E. Respond only with the the correct option. Do not include any explanations.\n",
      "Context:\n",
      "\n",
      "Document 0:1>\telse if the failure is detected due to Mobility from NR failure as described in 5.4.3.5, set the fields in VarRLF-report as follows:\n",
      "\n",
      "2>\tset the connectionFailureType to hof;\n",
      "\n",
      "2>\tif last MobilityFromNRCommand concerned a failed inter-RAT handover from NR to E-UTRA and if the UE supports Radio Link Failure Report for Inter-RAT MRO EUTRA (NR to EUTRA):\n",
      "Document 1:Release due to CN-detected mobility\n",
      "\n",
      "The context release is requested by the AMF because the UE is already served by another CN node (same or different system), or another NG interface of the same CN node.\n",
      "\n",
      "N26 interface not available\n",
      "\n",
      "The action failed due to a temporary failure of the N26 interface.\n",
      "\n",
      "Release due to pre-emption\n",
      "\n",
      "Release is initiated due to pre-emption.\n",
      "\n",
      "Multiple Location Reporting Reference ID Instances\n",
      "\n",
      "The action failed because multiple areas of interest are set with the same Location Reporting Reference ID.\n",
      "\n",
      "RSN not available for the UP\n",
      "\n",
      "The redundant user plane resources indicated by RSN are not available.\n",
      "\n",
      "NPN access denied\n",
      "\n",
      "Access was denied, or release is requested, for NPN reasons.\n",
      "\n",
      "CAG only access denied\n",
      "Document 2:\"Relocation failure\" is used by the target MME/S4-SGSN to indicate the source MME/S4-SGSN that the relocation has failed.\n",
      "Document 3:When the UE fails the mobility from NR procedure, it indicates the release of the RRC connection to upper layers together with the release cause 'RRC connection failure'.\n",
      "\n",
      "Question:\n",
      "What release cause is indicated when the UE fails the mobility from NR procedure?\n",
      "\n",
      "Options:\n",
      "A) 'Handover failure'\n",
      "B) 'RRC connection release'\n",
      "C) 'RRC connection failure'\n",
      "D) 'Mobility failure'\n",
      "E) 'Other'\n",
      "\n",
      "Output:\n",
      "C) 'RRC connection failure'\n",
      "Loading model\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:31<00:00, 15.87s/it]\n",
      "Training\n",
      "Converting eval dataset to ChatML: 100%|█| 181/181 [00:00<00:00, 1828.29 example\n",
      "Adding EOS to eval dataset: 100%|████| 181/181 [00:00<00:00, 4076.01 examples/s]\n",
      "Tokenizing eval dataset: 100%|████████| 181/181 [00:00<00:00, 909.85 examples/s]\n",
      "Truncating eval dataset: 100%|███████| 181/181 [00:00<00:00, 7167.92 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "🟢 No checkpoints found. Starting training from scratch.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 2.0379, 'grad_norm': 0.22820433974266052, 'learning_rate': 0.0001725995491923131, 'epoch': 1.3287671232876712}\n",
      "{'eval_loss': 1.798945426940918, 'eval_runtime': 17.5192, 'eval_samples_per_second': 10.331, 'eval_steps_per_second': 1.085, 'eval_num_tokens': 548360.0, 'eval_mean_token_accuracy': 0.6483603057108427, 'epoch': 1.3287671232876712}\n",
      "{'loss': 1.7767, 'grad_norm': 0.28298887610435486, 'learning_rate': 9.098469188879349e-05, 'epoch': 2.6575342465753424}\n",
      "{'eval_loss': 1.7323989868164062, 'eval_runtime': 17.8394, 'eval_samples_per_second': 10.146, 'eval_steps_per_second': 1.065, 'eval_num_tokens': 1081990.0, 'eval_mean_token_accuracy': 0.651862621307373, 'epoch': 2.6575342465753424}\n",
      "{'loss': 1.7145, 'grad_norm': 0.2496112585067749, 'learning_rate': 1.6231415137003537e-05, 'epoch': 3.9863013698630136}\n",
      "{'eval_loss': 1.7092013359069824, 'eval_runtime': 17.7622, 'eval_samples_per_second': 10.19, 'eval_steps_per_second': 1.07, 'eval_num_tokens': 1626658.0, 'eval_mean_token_accuracy': 0.6601616018696835, 'epoch': 3.9863013698630136}\n",
      "{'train_runtime': 1163.7325, 'train_samples_per_second': 3.111, 'train_steps_per_second': 0.077, 'train_loss': 1.8158759223090277, 'num_tokens': 1947091.0, 'mean_token_accuracy': 0.6412123058034086, 'epoch': 4.767123287671232}\n",
      "trainable parameters: 26214400\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mphi-2-rag-k3_teleqna-5e_10bs\u001b[0m at: \u001b[34mhttps://wandb.ai/dinho15971-unicamp/SBBD_phi-2-adapters/runs/0a2q7ld0\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250503_171222-0a2q7ld0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python  ../scripts/ft_phi.py \\\n",
    "  --new_model_name \"phi-2-rag-k3_teleqna-5e_10bs\" \\\n",
    "  --num_epochs 5 \\\n",
    "  --batch_size 10 \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --include_docs \\\n",
    "  --top_k 3 \\\n",
    "  --save_path \"../models/teleqna/adapters/\" \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vs_teleqna_150_20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac96d23-f5a1-4920-9f30-8a1813de35ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f044e5ac-831c-4972-9046-631da2a19fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model from LoRA adapter: ../models/teleqna/adapters/best_phi-2-rag-k3_teleqna-5e_10bs\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  3.60it/s]\n",
      "Load model embedding : ../models/teleqna/embedding/bge-small-3gpp_10e_128bs/checkpoint-30\n",
      "Using device: cuda\n",
      "💾 Vector store loaded from../vector_stores/teleqna/ft_vs_teleqna_150_20\n",
      "Device set to use cuda:0\n",
      "Loading dataset splits for teleqna\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Dataset({\n",
      "    features: ['question_id', 'question', 'A', 'B', 'C', 'D', 'E', 'answer', 'explanation', 'category', 'tag'],\n",
      "    num_rows: 905\n",
      "})\n",
      "🔍 Buscando: 100%|██████████████████████████████| 19/19 [00:03<00:00,  4.81it/s]\n",
      "Generating prompts: 100%|███████████████████| 905/905 [00:00<00:00, 6971.68it/s]\n",
      "Example prompt\n",
      "Instruct: Use the context provided below to answer the question. Select the correct option from A, B, C, D. Respond only with the the correct option. Do not include any explanations.\n",
      "Context:\n",
      "\n",
      "Document 1:The evaluation baseline includes at least NR R15 mandatory without capability features. Optional features from R15 onwards (e.g. CA, MIMO) as well as implementation-based energy saving techniques are to be explicitly reported and described if used in the evaluation baseline.\n",
      "\n",
      "SLS is considered as baseline evaluation method. Other method, including numerical analysis and LLS can also be considered. At least one of the methods is to be selected and used for evaluation of a specific technique (selection and criteria is up to proponent).\n",
      "Document 2:In the evaluation, the baseline operation and target operation for each source are as below:\n",
      "\n",
      "\tSource 1 ([19]) and Source 3 ([35]) choose Option 1:\n",
      "\n",
      "-\tBaseline operation for comparison: legacy static TDD {DDDSU} based on Rel-17 specifications\n",
      "\n",
      "-\tTarget flexible TDD operation: legacy static TDD {DSUUU} based on potential enhancements\n",
      "Document 3:-\tthe small scale parameters are according to clause 7.6.3.1 of TR 38.901\n",
      "\n",
      "-\tthe absolute time of arrival is according to clause 7.6.9 of TR 38.901\n",
      "\n",
      "Baseline evaluation does not incorporate spatially consistent UT/BS mobility modelling (clause 7.6.3.2 of TR 38.901). It is optional to implement it.\n",
      "\n",
      "Baseline for performance evaluation\n",
      "\n",
      "Existing Rel-16/Rel-17 positioning methods. Specific existing positioning method (e.g., DL-TDOA, Multi-RTT) used as comparison is to be reported.\n",
      "Document 4:An obvious baseline for assessing the radio performance is a transmitter / receiver performance that just meets RAN4 minimum requirements. However, it is to be expected that to a reasonable extent the performance could be extended beyond the minimum needed for meeting for 3GPP requirements and thus examples are provided considering both the baseline and some improvement. The associated complexity of improvement should be considered.\n",
      "Document 5:6.11.2\tSolution evaluation\n",
      "Document 6:The performance of Baseline HARQ-ACK (scheme 5.1 in Tables B.1.5-1,2,3) has been compared against different schemes with soft HARQ-ACK. Particularly, the following schemes, with soft HARQ-ACK, have been evaluated:\n",
      "Document 7:-\tevaluation results from 1 source show performance degradation in terms of the top-1 beam accuracy from 73.9% to 34.2% at 4 beams in Set B, from 88.6% to 63.9% at 8 beams in set B, from 97.8% to 88.4% at 16 beams in set B.\n",
      "\n",
      "-\tevaluation results from 5 sources show better performance than non-AI baseline option 2 (based on the measurements from Set B of beams). However, evaluation results from 5 sources similar or even worse performance than non-AI baseline option 2 (based on the measurements from Set B of beams).\n",
      "Document 8:-\tBaseline: \n",
      "\n",
      "-\tOptional: \n",
      "\n",
      "-\t is a dynamic part of power for BS in active, which is scaled based on reference configuration.\n",
      "\n",
      "-\tBaseline: , where ,, is the fraction of active TRxRUs, the ratio between the RF bandwidth and the maximum system BW, and the ratio of PSD per TxRU between the DL transmission and reference configuration, respectively.\n",
      "\n",
      "\n",
      "\n",
      "-\t is the power part related to PA.\n",
      "\n",
      "-\tFor simplicity \n",
      "\n",
      "-\tA = baseline: 0.4; optional: 0.1, 0.7;\n",
      "\n",
      "-\tFor , in evaluation, company to report the assumption from below:\n",
      "\n",
      "-\tIf one value of  is used for evaluation,  for any ;\n",
      "\n",
      "-\tIf two values of are used for evaluation,  if ; otherwise,\n",
      "Document 9:-\tThe UPT gains of the reference scheme(s) and proposed scheme over the baseline operation for comparison are presented in Table 8.3.1A.3-1, Table 8.3.1A.3-2, Table 8.3.1A.3-3, Table 8.3.1A.3-4 and Table 8.3.1A.3-5.\n",
      "Document 10:The baseline ACIR is derived from the ACLR and ACS values listed in Annex E, in Tables E.2.3-1, E.2.3-2, and E.2.4-1. On the other hand, the enhanced ACIR that RAN4 has considered in the coexistence study represents the case where the baseline ACIR is enhanced with x dB, ranging from 2dB till 8dB, while some numbers can’t be met because ACIR enhancement can be limited by the ACLR/ACS of the legacy TDD network or UE operating in the adjacent channel as highlighted below for the different cases:\n",
      "\n",
      "Question:\n",
      "Which evaluation method is considered as the baseline?\n",
      "\n",
      "Options:\n",
      "A) Numerical analysis\n",
      "B) SLS (Symbol Level Simulation)\n",
      "C) LLS (Link Level Simulation)\n",
      "D) All of the above\n",
      "\n",
      "Output:\n",
      "Processing inference\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2236 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "inference completed\n",
      "answer save in ../results/teleqna/full_ft_k10_teleqna.csv\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_rag.py \\\n",
    "  --lora_adapter_path \"../models/teleqna/adapters/best_phi-2-rag-k3_teleqna-5e_10bs\" \\\n",
    "  --max_new_tokens 40 \\\n",
    "  --vector_store_path \"../vector_stores/teleqna/ft_vs_teleqna_150_20\" \\\n",
    "  --dataset_name \"teleqna\" \\\n",
    "  --output_csv_path \"../results/teleqna/full_ft_k10_teleqna.csv\" \\\n",
    "  --bs_emb 50 \\\n",
    "  --bs_gen 10 \\\n",
    "  --top_k 10 \\\n",
    "  --use_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c26f53-f494-429a-9966-65d49e6a08e7",
   "metadata": {},
   "source": [
    "## Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a537228-dbee-418c-9c50-fff74df25ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset splits for teleqna\n",
      "Train: 724\n",
      "Val: 181\n",
      "Test: 905\n",
      "Datasets loaded and prepared.\n",
      "Results saved in: ../results/teleqna/full_ft_k10_teleqna.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluate_inference import evaluate_answer\n",
    "evaluate_answer('teleqna', '../results/teleqna/full_ft_k10_teleqna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ec7a1-57ec-46c3-9675-bce82393e792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (env_sbbd)",
   "language": "python",
   "name": "env_sbbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
